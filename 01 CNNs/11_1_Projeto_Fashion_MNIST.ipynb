{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1jaDz04lbzU+a5rqkjesH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Alunos**\n",
        "- Vandelson Elias (vemf@cesar.school)\n",
        "- JP Veloso (Jpgev@cesar.school)"
      ],
      "metadata": {
        "id": "UWstFj6dO2s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bibliotecas**"
      ],
      "metadata": {
        "id": "oTLjBwAnPrC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "QnUmLWiQsimf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o dispositivo (GPU se disponível, senão CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5-jz5HtCPXLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define o modelo da rede neural**"
      ],
      "metadata": {
        "id": "wV_SxJOuPbCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "hSslXgy3sgto",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Carrega o dataset Fashion MNIST**"
      ],
      "metadata": {
        "id": "x4MRx7xSQB-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.FashionMNIST('../data', train=True, download=True,\n",
        "                   transform=transform)\n",
        "dataset2 = datasets.FashionMNIST('../data', train=False,\n",
        "                   transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size = 64)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size = 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uln8qJ9kCpVR",
        "outputId": "948ee0ce-c8de-4e20-84d5-f4519a43f071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 175kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.31MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 22.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Treinamento e Avaliação do Modelo**"
      ],
      "metadata": {
        "id": "yXsIz16ZQPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma instância do modelo e move para o dispositivo\n",
        "model = Net().to(device)\n",
        "\n",
        "# Define a função de perda e otimizador\n",
        "# optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Treina o modelo\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "# Avalia o modelo\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agTGkadzoYtY",
        "outputId": "f14d7392-ade6-41fd-fd6c-799e68e4b7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301761\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.581459\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.388488\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.524741\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.454931\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.516063\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.352203\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.522451\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.406918\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.378471\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.286478\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.375344\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.254475\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.456930\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.337346\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.386357\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.223509\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.517391\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.320124\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.278912\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.208083\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.231539\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.255045\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.312848\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.300748\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.359812\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.240348\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.372546\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.285542\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.210756\n",
            "\n",
            "Test set: Average loss: 0.2541, Accuracy: 9092/10000 (91%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRHnCcdxpPwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}